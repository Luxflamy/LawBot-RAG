import streamlit as st
import os
import json
from dotenv import load_dotenv, find_dotenv
from langchain.chains import RetrievalQA
from langchain.document_loaders import Docx2txtLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate

# Configuration and loading
load_dotenv(find_dotenv())  # Load environment variables from .env file

# Document-specific configurations
with open("data/fakao_gpt4.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)

from langchain.docstore.document import Document
texts = [
    Document(page_content=item["output"], metadata={"question": item["input"], "type": item["type"]})
    for item in json_data
]

# Text splitting and embeddings
text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)
texts = text_splitter.split_documents(texts)
texts = texts[:500]  # é™åˆ¶æœ€å¤šå¤„ç†å‰ 500 æ¡æ–‡æ¡£å—
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002", api_key=os.environ["OPENAI_API_KEY"]
)

# Vector store and retriever
vector_store = vector_store = FAISS.from_documents(texts, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# Question-answering system
prompt_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡æ³•å¾‹å’¨è¯¢åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹çŸ¥è¯†å†…å®¹ï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚

æ³•å¾‹å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

è¯·ç”¨ç®€æ´ã€æ­£å¼ã€ä¸“ä¸šçš„ä¸­æ–‡å›ç­”ã€‚å¦‚æœæ— æ³•ç¡®å®šç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
"""

QA_PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa = RetrievalQA.from_chain_type(
    llm=OpenAI(), 
    chain_type="stuff", 
    retriever=retriever, 
    chain_type_kwargs={"prompt": QA_PROMPT}
)
llm = OpenAI(temperature=0.5, max_tokens=1000)

def rag_answer(query):
    try:
        response = qa.run(query)
        return f"ã€RAGæ¨¡å¼å›ç­”ã€‘ï¼š\n{response}"
    except Exception as e:
        return f"RAGæ¨¡å¼å‡ºé”™ï¼š{e}"

def prompt_only_answer(query):
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡æ³•å¾‹å’¨è¯¢åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜æä¾›æƒå¨è§£ç­”ï¼š

é—®é¢˜ï¼š{query}

è¯·ä»¥â€œæ ¹æ®æˆ‘å›½ç°è¡Œæ³•å¾‹è§„å®šâ€¦â€å¼€å¤´å›ç­”ã€‚"""
    try:
        response = llm(prompt)
        return f"ã€Promptæ¨¡å¼å›ç­”ã€‘ï¼š\n{response}"
    except Exception as e:
        return f"Promptæ¨¡å¼å‡ºé”™ï¼š{e}"

def main():
    st.set_page_config(page_title="æ³•å¾‹æœåŠ¡åŠ©æ‰‹")

    st.title("ğŸ§‘â€âš–ï¸ AI æ³•å¾‹å’¨è¯¢åŠ©æ‰‹")
    st.markdown("è¯·é€‰æ‹©æ¨¡å¼ï¼Œå¹¶è¾“å…¥æ‚¨è¦å’¨è¯¢çš„æ³•å¾‹é—®é¢˜ï¼š")

    # æ¨¡å¼é€‰æ‹©
    mode = st.radio("é—®ç­”æ¨¡å¼", ["RAG æ¨¡å¼ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ï¼‰", "Prompt-only æ¨¡å¼"])

    query = st.text_input("è¯·è¾“å…¥é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šå…¬å¸è¾é€€å‘˜å·¥æ˜¯å¦éœ€è¦èµ”å¿ï¼Ÿ")

    if query:
        if mode == "RAG æ¨¡å¼ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ï¼‰":
            response = rag_answer(query)
        else:
            response = prompt_only_answer(query)

        st.markdown("### ğŸ’¡ å›ç­”ï¼š")
        st.write(response)

if __name__ == "__main__":
    main()
